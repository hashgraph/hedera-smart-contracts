---
status: "proposed"
date: "2025-10-07"
decision-makers: Fernando Paris [fernando.paris@swirldslabs.com](mailto:fernando.paris@swirldslabs.com), Mariusz Jasuwienas [mariusz.jasuwienas@arianelabs.com](mailto:mariusz.jasuwienas@arianelabs.com), Michal Walczak [michal.walczak@arianelabs.com](mailto:michal.walczak@arianelabs.com), Piotr Swierzy [piotr.swierzy@arianelabs.com](mailto:piotr.swierzy@arianelabs.com)
consulted:
informed:
---

# ADR 005: Automate documentation generation for System Contracts

## Context and problem statement

Currently, the **System Contracts interfaces** (e.g., `IHederaTokenService`) contain limited inline documentation - typically only a brief summary per method.
This creates friction for developers trying to understand how these contracts map to **system contract calls**.

Manually maintaining the documentation is time-consuming.
Consensus node's code still evolves so it may become outdated again.
To ensure accuracy and consistency, we should **automatically generate documentation** for these interfaces based on canonical **protobuf definitions** and **consensus node specifications** located in the `.proto` files in the [hiero-consensus-node](https://github.com/hiero-ledger/hiero-consensus-node) repository.

> WARNING! Unfortunately currently, only the gRPC API interface is well-documented; precompiles and their mappings are not documented at all. Only the services executed when a precompile address with a certain selector is called are documented as well.

### Clarified explanation

`{Action}CallTranslator` -> `identifyMethod` calls `{Action}Decoder`.

For example:
`MintTranslator` -> `MintDecoder`

The decoder takes the raw input bytes and builds a `TransactionBody` object - for instance, a `TokenMintTransactionBody`.
We already have protobuf definitions for these `TransactionBody` types. `TokenMintTransactionBody` is the argument that the **HTS (Hedera Token Service)** uses in the gRPC method call `TokenMint`.
That’s exactly the structure we’re trying to identify.

However, at the **contract service communication level** (where all EVM transactions are processed), the input comes in as **EVM transaction input data** (`inputData`).

From that data, we extract:

* the **target address (`to`)**, and
* the **method selector** and params.

Then, we look up the selector in a registry of translators.

Each translator is **registered** by adding a key-value pair in `SystemContractMethodRegistry`,
where the key is the **method selector**, and the value is the **Translator** instance.

You can see an example of this in `MintTranslator.java`:

```java
public static final SystemContractMethod MINT = SystemContractMethod.declare(
        "mintToken(address,uint64,bytes[])", "(int64,int64,int64[])")
    .withVariant(Variant.V1)
    .withCategories(Category.MINT_BURN);
(...)
registerMethods(MINT, MINT_V2);
```

The **selector** is taken from `MINT`.

`registerMethods()` is called when a singleton instance of the corresponding module (e.g., `HtsTranslatorModules`) is created.

### How to find all available methods

To gather a complete list of all methods:

* Check the directory:
  `src/main/java/exec/processors/*Module`
* Look for **imported translators** (the simplest way might be to scan the imports).

You can identify translators by the annotation:
`@Named("HasTranslators")` -
which indicates the “Has” method and tells you which system contract it is associated with.

Inside each translator, look for `Method.signature`.
Note that there might be **multiple signatures mapped to a single method** — for example, a precompile might handle:

```text
mintToken(address,uint64,bytes[])
mintToken(address,int64,bytes[])
```

But both map to the same `MintTransactionBody` format in HTS.
The HTS service accepts `uint64` and ignores the original EVM-level type (`uint64` vs `int64`).

### Twist

Sometimes, like in the case of:

```text
hbarAllowance(address)
```

there’s **no decoder** and **no call** to HTS or any other service at all.

Instead, the translator itself directly returns the value (for example, an allowance balance).
Here’s a simplified pseudocode example:

```
fn exec(evmTx):
  for translator in translators:
    if translator.selector == evmTx.inputData.selector:
      return translator.exec(evmTx)

class HBARAllowanceTranslator:
  selector = 'hbarAllowance(address)'
  fn exec(evmTx):
    return evmTx.from.allowance(evmTx.inputData.arg)
```

In such cases, the most relevant protobuf would be the one associated with the `Account` object (`account.proto`).
But it’s hard to determine this automatically - translators can be custom and inconsistent, so there’s no guaranteed pattern.


### Typical case (straightforward example)

Usually, it looks like this:

```
class MintTranslator:
  selector = 'mint(...)'
  decoder = MintDecoder
  fn exec(evmTx):
    return MintDecoder.decode(evmTx)

class MintDecoder:
  fn decode(evmTx):
    return MintTransactionBody(...)
```

In this setup, you can **trace the logic** step-by-step to the corresponding `MintTransactionBody`, which has its protobuf definition.


### Drawbacks

It’s easy to break this pattern:

* A **translator** might not have a decoder.
* It might have **multiple decoders**, or name them differently.
* Even if it does have a `decode()` method, it might manipulate data arbitrarily.

For example:

```python
fn decode(evmTx):
  return new MintTransactionBody(
      evmTx.arg1,
      evmTx.arg2,
      castToSomethingElse(evmTx.arg4)
  )  # ignored arg3, casted arg4
```

So while this structure gives a general idea of how translators and decoders map EVM input data to protobuf transaction bodies, there are many edge cases and custom implementations that make it unreliable to automate fully.

## Decision Drivers

* Maintain 1:1 consistency between Solidity interfaces and consensus node code
* Reduce manual maintenance and update burden
* Improve developer experience thanks to the richer and more accurate docs
* Support auto-generation of Markdown and NatSpec for use in IDEs and NPM docs

## Considered options

1. Use [`protoc-gen-doc`](https://github.com/pseudomuto/protoc-gen-doc)
2. Write a Custom Documentation Generator
3. Use the Solidity files from the Hedera Consensus Node repository and make sure they are always well-documented
4. **One time semi-automatic documentation generation - worry about keeping it up to date later** (recommended)

## Option 1: Use [`protoc-gen-doc`](https://github.com/pseudomuto/protoc-gen-doc)

These are `protoc` plugins that generate documentation and metadata from `.proto` files, supporting structured output formats (Markdown, JSON, HTML, etc.).
We could use them to generate Markdown files describing our System Contracts and store them directly in our documentation directory.

### Positive

* Enables easy conversion of protobuf files into Markdown or HTML documentation.
* Minimal setup effort, leveraging existing open-source tooling.

### Negative

* Protobuf files in the consensus node repository do not strictly match the interfaces of the precompiles.
* We would need to **map the `TransactionBody` protobuf definitions** describing actual calls into the corresponding **smart contract methods** before the output becomes useful.
* Limited flexibility to customize documentation for Solidity - specific details.

## Option 2: Write a custom documentation generator

Develop an internal tool to directly parse protobuf definitions and generate documentation artifacts such as:

* Solidity docstrings (NatSpec format)
* Markdown developer documentation

This tool could leverage `protobufjs` (TypeScript) to parse definitions and render output using templates.

### Positive

* The initial PoC was already prepared during the verification process, so we know it’s possible.
* Full control over mapping logic and formatting.
* Enables defining a clear mapping between Java transaction interfaces and Solidity method signatures - which would be necessary in other approaches as well.
* Final output can be generated in multiple formats (both: NatSpec and Markdown), not limited to a specific format.

### Negative

* Slightly higher initial development cost.
* Requires defining and maintaining a mapping layer between protobuf and Solidity interfaces.

## Option 3: Use the solidity files from the Hedera consensus node repository

Since the functionality and behavior of precompiles depend directly on how the code is implemented in the consensus node repository,
it might make sense for the **canonical and up-to-date documentation** to live there.

This repository already exposes internal services and example precompile interfaces that are similar to those in the `hedera-smart-contracts` repository but often more up to date.

### Positive

* Simplest solution - minimal new work required.
* Documentation responsibility could be shifted to the repository that directly implements the precompiles.
* Developers could be redirected to view the code and documentation in the consensus node repository for better references.

### Negative

* Requires discipline and additional effort from the team maintaining the consensus node code.
* Less control from the System Contracts repository over the documentation format and availability.

## Option 4: One-time semi-automatic documentation generation

Generate the documentation **once**, using a semi-automated approach (for example, by combining `protoc` output, manual editing, and scripted Markdown generation), and defer automation of updates until later.
This would allow us to quickly produce an initial set of documentation for all System Contracts without committing to a fully automated pipeline yet.

### Positive

* **Fast initial delivery** - produces usable documentation quickly with minimal setup.
* **Low implementation effort** - can use existing tools and scripts without developing or integrating a new generator.
* **Good starting point** - provides a baseline that can be iteratively improved or automated later.

### Negative

* **Documentation will quickly become outdated** as protobufs or node interfaces evolve.
* **Requires manual updates** in future releases, risking inconsistencies between code and docs.
* **No long-term scalability** - effectively a temporary solution that delays the need for full automation.

## Decision Outcome

Since the tool has mapping issues and high complexity (and would never be completely error-proof anyway), the safest and best approach is to generate the documentation once.
